{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises on numpy\n",
    "\n",
    "This is 8 short exercises on variables.\n",
    "\n",
    "Use this [notebook](https://github.com/dtaantwerp/dtaantwerp.github.io/blob/master/notebooks2021/1_Week1_Monday_Python_and_Variables.ipynb) for a complete explanation of numpy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. How to read a line?!\n",
    "Lines are functions of the form $f(x) = mx + q$, with $m$ and $q$ being the `slope` and `intercept` respectively.\n",
    "\n",
    "If this is still unfamiliar to you, play with [this](https://www.desmos.com/calculator/59qdbtnlzy) for a while. See how changing the slope and intercept affects the shape and behavior of a line. Note that for a line which meets the horizontal and vertical axis at $x_0$ and $y_0$ respectively, $-\\frac{y_0}{x_0}$ is basically the slope!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. A very very simple (yet crucial) introduction to Machine Learning!\n",
    "One of the main tasks in Machine Learning is Classification: you have some data (x), you have some classes or categories or labels (y), you want the machine to learn which data point belongs to which class. An example in sentiment analysis is that you have some product reviews and you want to classify them into Positive or Negative sentiment. If you can visualize your data, this might be how it looks in 2D, where classes are identified with color; e.g. the blue points are positive reviews and the orange points are the negative ones: \n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2020/03/Scatter-Plot-of-Binary-Classification-Dataset-With-2D-Feature-Space.png\" width=\"400\" /> \n",
    "\n",
    "Now imagine that for this data, we want to learn a `linear` classifier, i.e. a LINE that is able to separate the two classes in a good (enough) way. This might not be always possible but looking at our data here, it seems like a pretty good choice. \\\n",
    "> Q: Which one of the following lines can be a good candidate for our classifier? \n",
    "\n",
    "* $y = 1.6x + 15$\n",
    "* $y = -15x + 5$\n",
    "* $y = -1.7x + 15$\n",
    "* $y = 5x + 10$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. (Gradient) descent from a mountain\n",
    "In your class notebook you saw an image like this (ignore the \"03\"!):\n",
    "\n",
    "<img src=\"https://blog.paperspace.com/content/images/size/w1750/2019/09/F1-03.large.jpg\" width=\"400\" /> \n",
    "\n",
    "Which represents the \"error landscape\" in a problem; i.e. how the error value varies when we change our parameters. Solving most (if not all) machine learning problems basically boils down to finding the minimum of an error function. Unfortunately we rarely have such an overall nice view of our function because it usually has thousands (or millions or billions!) of parameters. In this situation, solving such a problem is kinda like descending a mountain with closed eyes! \n",
    "One logical way to do this is feeling the ground with your feet to see which way goes down (or goes down sharper). If you want to turn this into an instruction, it would be something like this:\n",
    "At any moment:\n",
    "* Feel the ground around you with your feet and try to have an estimate of the slope in different directions.\n",
    "* Move a bit in the direction which has the sharpest negative slope.\n",
    "* Repeat. \\\n",
    "\\\n",
    "\\\n",
    "This is the main idea behind the Gradient Descent algorithm, which basically goes like this:\n",
    "* Start somewhere on the error landscape. If you can, take a smart guess instead a random choice!\n",
    "* Calculate the derivatives along different directions (this is called the Gradient vector).\n",
    "* Move in the opposite direction of this vector (because it points towards higher values), with a step which is proportional to the slope value (i.e. you take bigger steps when the slope is sharper and vice versa)  \\\n",
    "\\\n",
    "\\\n",
    "Let's see this in action:\n",
    "* Go [here](https://uclaacm.github.io/gradient-descent-visualiser/#playground)\n",
    "* Copy-paste this under `Function` : .01x^4 - x^2 + 2x  . This is our error (or cost) function which we want to descent from; i.e. we are looking for its lowest value but -as mentioned before- we don't have this view of the function. We start blindfolded from somewhere and try to slowly move towards lowest (lower) point(s).\n",
    "> Q: Give an estimate for $min(f)$ and $argmin(f)$ where $f$ is our error function.\n",
    "* Let's assume that we start from $x = 12$. So put 12 under `Starting Point`.\n",
    "* Now click on `Set Up`. You should see the function and the tangent line at the starting point (red dot) on the chart. Adjust your view of the chart (if necessary) by scrolling on it.\n",
    "* We are almost ready for our descent but before that, we should decide on a value for our step (or `Learning Rate`)! Are we going to -generally- take small steps or big ones? For now, just put 0.01 under it. (Note that we will multiply this by the slope value).\\\n",
    "> Q: Where do you think you will end up? In the right valley? Left valley? Middle hill? Somewhere else?\n",
    "* Start the descent by clicking on `Next Iteration`. Watch how your location (red point on the chart) changes after each iteration (or step).\n",
    "* Keep clicking! Note how your movements become smaller and smaller as you come closer to the valley. (Q: Why is that?)\\\n",
    "\\\n",
    "> Q: After a while, it seems that you are stuck in the right valley, which is not the lowest point of the landscape. Do you think more clicking will get you out? Why?\n",
    "* Now let's descent again, but this time with a different starting point, say $x = -12$. Change the `Starting Point`, click on `Set Up` and repeat the experiment. See how it goes. \\\n",
    "As you witnessed, where you start your descent from makes a big difference in where you end up. Bad news is we are blindfolded; we don't have the overall view, therefore if we go down the mountain guided by the Gradient Descent algorithm, we (almost) always end up in a Local Minimum instead of the Global Minimum. Good news is there are ways to improve the algorithm. Even better, by the power of God or some other supernatural being, local minima often are good enough to base our model on. (Note that for real error functions, the landscape is a super hilly one with millions or billions of valleys!) If this was not the case, Machine Learning as we know today, didn't exist!   \n",
    "\n",
    "> Q: Play with the tool. Try different starting points, learning rates and functions. Do you think there could be a general receipe for the value of `Learning Rate`? What is good (and bad) about picking a small learning rate?\n",
    "  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The coordinates of your smile!\n",
    "Any digital image is an array of pixels. For a black&white image (not grayscale!), this array is a binary one, with the values telling us if that pixel should be On (1) or Off (0). Here is the smiley symbol as an 20x20 array of B&W pixels: \\\n",
    "\\\n",
    "<img src=\"https://github.com/dtaantwerp/dtaantwerp.github.io/tree/master/exercises2021/img/smiley.png\" width=\"150\" /> \n",
    "\\\n",
    "\\\n",
    "Let's create this array in Python so that we can play with it! Here is a boring way to do so:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run the cell\n",
    "import numpy as np\n",
    "\n",
    "a0 = [0]*20\n",
    "a1 = a0\n",
    "a2 = [0]*7 + [1]*6 + [0]*7\n",
    "a3 = [0]*6 + [1] + [0]*6 + [1] + [0]*6\n",
    "a4 = [0]*4 + [1,1] + [0]*8 + [1,1] + [0]*4\n",
    "a5 = [0]*4 + [1] + [0]*10 + [1] + [0]*4\n",
    "a6 = [0]*3 + [1] + [0]*3 + [1,1] + [0,0] + [1,1] + [0]*3 + [1] + [0]*3\n",
    "a7 = [0]*2 + [1] + [0]*4 + [1,1] + [0,0] + [1,1] + [0]*4 + [1] + [0]*2\n",
    "a8 = [0]*2 + [1] + [0]*14 + [1] + [0]*2\n",
    "a9 = [0]*2 + [1] + [0]*14 + [1] + [0]*2\n",
    "a10 = [0]*2 + [1] + [0]*14 + [1] + [0]*2\n",
    "a11 = [0]*2 + [1] + [0]*2 + [1] + [0]*8 + [1] + [0]*2 + [1] + [0]*2\n",
    "a12 = [0]*2 + [1] + [0]*3 + [1] + [0]*6 + [1] + [0]*3 + [1] + [0]*2\n",
    "a13 = [0]*3 + [1] + [0]*3 + [1]*6 + [0]*3 + [1] + [0]*3\n",
    "a14 = [0]*4 + [1] + [0]*10 + [1] + [0]*4\n",
    "a15 = [0]*4 + [1,1] + [0]*8 + [1,1] + [0]*4\n",
    "a16 = [0]*6 + [1] + [0]*6 + [1] + [0]*6\n",
    "a17 = a2\n",
    "a18 = a0\n",
    "a19 = a0\n",
    "\n",
    "smiley = [a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19]\n",
    "smiley = np.array(smiley)\n",
    "smiley"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can actually display the array as an image using some Python libraries. Here is a way:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install matplotlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(smiley, cmap='Greys')\n",
    "plt.xticks(np.arange(0.5, 20.5, 1), labels=[])\n",
    "plt.yticks(np.arange(0.5, 20.5, 1), labels=[])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Q: Identify the left and right eye of the image as a selection of this array. \\\n",
    "> Q: Imagine a (rectangular) nose for the image. Identify this hypothetical nose as a selection of the array. (you can choose the size or orientation!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "left_eye = smiley[\"\"\"...your code here...\"\"\"]\n",
    "right_eye = smiley[\"\"\"...your code here...\"\"\"]\n",
    "nose = smiley[\"\"\"...your code here...\"\"\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the array to modify the image. Let's start by removing the left eye! which is equivalent to setting the left-eye pixels to 0.\n",
    "> Q: Do this by turning off the left-eye pixels one-by-one:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "smiley[???,???] = 0\n",
    "smiley[???,???] = 0\n",
    "smiley[???,???] = 0\n",
    "smiley[???,???] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how it looks like now. Since we are going to use the display cell frequently, let's turn it into a function that receives an array and displays it as a B&W image. For simplicity you can just assume that the arrays are always 20x20 so that you don't need to change the grid lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_array(array):\n",
    "    # your code here\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can just call it on our array to see how it looks:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_array(smiley)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poor smiley! Let's put the eye back but in a less boring way! \\\n",
    "We can do it all at once, by assigning the whole left-eye area (as a selection of the array) to an array of the same size with values of 1: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's create the eye separately ...\n",
    "eye = np.ones(\"\"\"...your code here...\"\"\")\n",
    "\n",
    "# and then assign it to the left-eye area:\n",
    "smiley[\"\"\"...your code here...\"\"\"] = eye\n",
    "\n",
    "\n",
    "display_array(smiley)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! let's add the nose in the same way:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nose = np.ones(\"\"\"...your code here...\"\"\")\n",
    "\n",
    "smiley[\"\"\"...your code here...\"\"\"] = nose\n",
    "\n",
    "display_array(smiley)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cool! Play more if you like! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. How accurate is your classifier?\n",
    "Imagine that we have a classifier for binary classification (i.e. there are only two classes in our data, which we call 0 and 1). We have applied this classifier on some labeled data,  got some predictions and now we want to see how accurate they are. We define accuracy simply as the ratio of correct predictions, so: $Acc = \\frac{Number-of-correct-predictions}{Total-number-of-predictions}$. \\\n",
    "Since the model is imaginary, we need to create hypothetical lists of predictions and labels.\n",
    "> Q: In numpy there is an easy way to do this. Check out [this](https://numpy.org/doc/1.13/reference/generated/numpy.random.randint.html) page and write a code that creates two random 1D binary arrays of size 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "labels = # your code here\n",
    "predictions = # your code here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But what if we didn't know about this `randint`? Can we do the same using only the random.random() method? \n",
    "Let's try! We know that random.random() returns random float values from the [0,1) interval. Like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "random_array = np.random.random(10)\n",
    "random_array"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.29528067, 0.18261099, 0.57663449, 0.8677704 , 0.09347673,\n",
       "       0.18896265, 0.35688493, 0.28501031, 0.8687269 , 0.43049414])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now if we deduct 0.5 from such values, they will be mapped into the [-.5, .5) interval. And if we round them up, they will be either 0. or 1.! \\\n",
    "Cool! Let's do this. First we deduct 0.5. \n",
    "> Q: What do you think about the following code? Should it work? Run and see!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "float_labels = random_array - .5\n",
    "float_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're not surprised, see what happens when you have your data as a `list` (instead of an `array`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_list = list(random_array)\n",
    "random_list - .5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You get a TypeError -*unsupported operand type(s) for -: 'list' and 'float'*- saying that the \"-\" operand can't operate between a list and a float. So why does it work with an array? \\\n",
    "This is called `Broadcasting` and is a super useful feature in NumPy (not exclusive to it!) You can read more about it [here](https://numpy.org/devdocs/user/basics.broadcasting.html) if you like but what it basically does is trying to `match` mismatched arrays involved in an operation, by stretching the smaller one through repetition! Naturally there are rules:\n",
    "> *When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when 1) they are equal, OR 2) one of them is 1.*   \n",
    "In the second case, instead of throwing an error, NumPy repeats the smaller array along the dimension(s) until it becomes of the same size as the bigger one. In our case for example, NumPy converts 0.5 to $array([0.5]*10)$ before applying the \"-\" operator. \\\n",
    "Back to our classifier. Now we need to round up the numbers which can be easily done using the ceil() method:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "binary_labels = np.ceil(float_labels)\n",
    "binary_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we convert them to integers to have clean binaries:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "binary_labels = binary_labels.astype(int)\n",
    "binary_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting them all together:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "labels = np.ceil(np.random.random(10) - .5).astype(int)\n",
    "predictions = np.ceil(np.random.random(10) - .5).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have our artificial arrays and we need to compare them to calculate the accuracy. If you were working with lists, you probably needed to loop over the elements and compare them one by one. But since we have arrays, we can do easier/faster/better! \\\n",
    "Let's see what happens when you check the equality of two arrays, as a whole!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels == predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you see, instead of comparing the whole arrays, NumPy compares them element by element, and returns the result as a Boolean array of `True` and `False` values. \\\n",
    "Now all we need to do is counting the number of `True`s in this array and dividing it by its length, for which we can use the count() method for lists:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compare = list(labels == predictions)\n",
    "accuracy = compare.count(True)/len(compare)\n",
    "accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the accuracy of our imaginary classifier! \\\n",
    "You can also do it all with arrays, like this :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "compare = (labels == predictions)\n",
    "accuracy = compare.mean()\n",
    "accuracy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here NumPy treats the Boolean array as a binary (True = 1, False = 0) and calculates the mean of it. Quite convenient!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Q: Go back to the smiley excercise. See if you can use `Broadcasting` to make the modifications even easier!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('perl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "5db9622ea32ad59ad8958f78283f4771a508012702d3a535a15489e1434ecdc6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}