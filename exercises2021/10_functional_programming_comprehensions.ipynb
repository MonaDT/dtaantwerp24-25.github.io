{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bronze-medium",
   "metadata": {},
   "source": [
    "# Functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-litigation",
   "metadata": {},
   "source": [
    "**Staff**: Jens Lemmens\n",
    "\n",
    "**Support material**: Class [notebook](https://github.com/dtaantwerp/dtaantwerp.github.io/blob/master/notebooks2021/16_Week3_Monday_functional_programming___list_comprehensions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-argument",
   "metadata": {},
   "source": [
    "## Comprehensions\n",
    "#### Rewrite the following lines of code as comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-emergency",
   "metadata": {},
   "source": [
    "#### Exercise 1: Create a list of numbers from 1 to 100, result is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "numbers = []\n",
    "for i in range(1,101):\n",
    "    numbers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legislative-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehension alternative\n",
    "numbers = [i for i in range(1, 101)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-orbit",
   "metadata": {},
   "source": [
    "#### Exercise 2: Split a sentence into tokens and uppercase the first letter of each word, result is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "designing-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a tokenized and lowercased sentence .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tested-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Is', 'A', 'Tokenized', 'And', 'Lowercased', 'Sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "tokenized_sentence = []\n",
    "for token in sentence.split():\n",
    "    tokenized_sentence.append(token.title())\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "former-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Is', 'A', 'Tokenized', 'And', 'Lowercased', 'Sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_sentence = [t.title() for t in sentence.split()]\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-assembly",
   "metadata": {},
   "source": [
    "#### Exercise 3: Tokenize a corpus, result is a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historic-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is a sentence .\", \"This is another sentence .\", \"This is the final sentence .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', 'sentence', '.', 'This', 'is', 'the', 'final', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "tokenized_corpus = []\n",
    "for s in sentences:\n",
    "    for t in s.split():\n",
    "        tokenized_corpus.append(t)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acoustic-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', 'sentence', '.', 'This', 'is', 'the', 'final', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_corpus = [t for s in sentences for t in s.split()]\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-label",
   "metadata": {},
   "source": [
    "#### Exercise 4: Tokenize a corpus, but keep only words longer than 3 characters, result is a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continuous-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is a sentence .\", \"This is another sentence .\", \"This is the final sentence .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collectible-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'This', 'another', 'sentence', 'This', 'final', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "tokenized_corpus = []\n",
    "for s in sentences:\n",
    "    for t in s.split():\n",
    "        if len(t) > 3:\n",
    "            tokenized_corpus.append(t)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "automotive-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'This', 'another', 'sentence', 'This', 'final', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_corpus = [t for s in sentences for t in s.split() if len(t) > 3]\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-signal",
   "metadata": {},
   "source": [
    "#### Exercise 5: Tokenize a corpus, but keep only words longer than 3 characters. Tokens that are not longer than 3 characters should be replaced with the string 'stop_word'. Result is a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crazy-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is a sentence .\", \"This is another sentence .\", \"This is the final sentence .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "premier-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'stop_word', 'stop_word', 'sentence', 'stop_word', 'This', 'stop_word', 'another', 'sentence', 'stop_word', 'This', 'stop_word', 'stop_word', 'final', 'sentence', 'stop_word']\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "tokenized_corpus = []\n",
    "for s in sentences:\n",
    "    for t in s.split():\n",
    "        if len(t) > 3:\n",
    "            tokenized_corpus.append(t)\n",
    "        else:\n",
    "            tokenized_corpus.append('stop_word')\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "central-cocktail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'stop_word', 'stop_word', 'sentence', 'stop_word', 'This', 'stop_word', 'another', 'sentence', 'stop_word', 'This', 'stop_word', 'stop_word', 'final', 'sentence', 'stop_word']\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_corpus = [t if len(t) > 3 else 'stop_word' for s in sentences for t in s.split()]\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-karen",
   "metadata": {},
   "source": [
    "#### Exercise 6: Tokenize a corpus, result is a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accepting-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is a sentence .\", \"This is another sentence .\", \"This is the final sentence .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fatal-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'a', 'sentence', '.'], ['This', 'is', 'another', 'sentence', '.'], ['This', 'is', 'the', 'final', 'sentence', '.']]\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "tokenized_sentences = []\n",
    "for s in sentences:\n",
    "    tokenized_s = s.split()\n",
    "    tokenized_sentences.append(tokenized_s)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appreciated-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'a', 'sentence', '.'], ['This', 'is', 'another', 'sentence', '.'], ['This', 'is', 'the', 'final', 'sentence', '.']]\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_sentences = [[t for t in s.split()]for s in sentences]\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-spectacular",
   "metadata": {},
   "source": [
    "#### Exercise 7: Split a sentence into tokens and map them to position indices, result is a dictionary with indices as keys and tokens as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlimited-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a tokenized sentence with token indices .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "according-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'This', 1: 'is', 2: 'a', 3: 'tokenized', 4: 'sentence', 5: 'with', 6: 'token', 7: 'indices', 8: '.'}\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "tokenized_sentence = dict()\n",
    "for i, token in enumerate(sentence.split()):\n",
    "    tokenized_sentence[i] = token\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "victorian-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'This', 1: 'is', 2: 'a', 3: 'tokenized', 4: 'sentence', 5: 'with', 6: 'token', 7: 'indices', 8: '.'}\n"
     ]
    }
   ],
   "source": [
    "# comprehension alternative\n",
    "tokenized_sentence = {i:token for i, token in enumerate(sentence.split())}\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-mills",
   "metadata": {},
   "source": [
    "#### Exercise 8: Zip a list of even numbers and a list of odd numbers from 0 to 100\n",
    "- First create a list of even numbers between 0 and 100 using range() in a list comprehension\n",
    "- Then do the same for odd numbers\n",
    "- Finally, create a new list that contains the sum of the zipped numbers in the two lists created above, but only if that sum is divisible by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "designed-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "even = [n for n in range(0,101) if n%2==0]\n",
    "odd = [n for n in range(0,101) if n%2]\n",
    "numbers = [e+o for e,o in zip(even, odd) if (e+o)%3==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "several-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 21, 33, 45, 57, 69, 81, 93, 105, 117, 129, 141, 153, 165, 177, 189]\n"
     ]
    }
   ],
   "source": [
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-despite",
   "metadata": {},
   "source": [
    "## Functions as function arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-revelation",
   "metadata": {},
   "source": [
    "### Self-written functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-mechanics",
   "metadata": {},
   "source": [
    "#### Write a function that takes a numpy array of numbers and a mathematical function (multiply or divide) as its arguments\n",
    "- Import numpy and create an array to test the function on\n",
    "- Ask the user the question \"Would you like to multiply or divide the input numbers?\"\n",
    "- Ask the user the question \"By which number?\"\n",
    "- Write three functions\n",
    "    - \"Divide\", which prints the array divided by the input number\n",
    "    - \"Multiply\", which prints the array multiplied by the input number\n",
    "    - \"Calculate\", which performs an operation on the array and takes either of the above functions as argument (use booleans and conditional statements to determine which function this should be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "running-fortune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numbers = np.array(range(1,11))\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "operation = input(\"Would you like to divide or multiply the input numbers? \")\n",
    "n = int(input(\"By which number? \"))\n",
    "\n",
    "#define mathematical functions\n",
    "def divide(numbers, n):\n",
    "    print(np.divide(numbers, n))\n",
    "\n",
    "def multiply(numbers, n):\n",
    "    print(np.multiply(numbers, n))\n",
    "\n",
    "def calculate(numbers, n, operation):\n",
    "    return operation(numbers, n)\n",
    "\n",
    "#determine which function should be used\n",
    "d = False\n",
    "m = False\n",
    "\n",
    "if operation.lower().strip() == 'divide':\n",
    "    d = True\n",
    "elif operation.lower().strip() == 'multiply':\n",
    "    m = True\n",
    "else:\n",
    "    print('Please provide a valid operation (divide/multiply)')\n",
    "\n",
    "if d:\n",
    "    calculate(numbers, n, divide)\n",
    "elif m:\n",
    "    calculate(numbers, n, multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-state",
   "metadata": {},
   "source": [
    "### Predifined functions: Filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-question",
   "metadata": {},
   "source": [
    "#### Exercise 1: Make a filter that extracts all vowels from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "upper-stevens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x0000027EBEF8D2C8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_vowel(s):\n",
    "    if s in ['a', 'e', 'i', 'o', 'u']: #if re.match([aeiou], s)\n",
    "        return True\n",
    "\n",
    "f = filter(is_vowel, 'Supercalifragilisticexpialidocious')\n",
    "\n",
    "print(f)\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-devon",
   "metadata": {},
   "source": [
    "#### Exercise 2: Make a filter that extracts all words longer than 5 characters from a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "great-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x0000027EBEF8D7C8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chocolates']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_long(t):\n",
    "    if len(t) > 5:\n",
    "        return True\n",
    "\n",
    "f = filter(is_long, \"Life is a box of chocolates , you never know what you are going to get .\".split())\n",
    "\n",
    "print(f)\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-batman",
   "metadata": {},
   "source": [
    "### Predifined functions: Map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-perfume",
   "metadata": {},
   "source": [
    "#### Exercise 1: Create a function that takes a number as argument and returns the square of that number. Use that function to map a list of numbers to a list of the square of those numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "changed-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = list(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "scientific-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(n):\n",
    "    return n*n\n",
    "\n",
    "m = map(square, nums) # m = map(lambda x: x*x, nums)\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-throat",
   "metadata": {},
   "source": [
    "#### Exercise 2: Create a function that takes a string as argument and returns its length. Use that function to map a list of strings to a list of the lengths of those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "apparent-material",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 1, 9, 8, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"This is a tokenized sentence .\".split()\n",
    "\n",
    "def get_len(string):\n",
    "    return len(string)\n",
    "\n",
    "m = map(get_len, x) # map(len, x)\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-concrete",
   "metadata": {},
   "source": [
    "## Lambda Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-workplace",
   "metadata": {},
   "source": [
    "#### Exercise 1: Create a lambda function that accepts two integers as input and returns the product of those integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fifty-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x, y: x*y\n",
    "f(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-utility",
   "metadata": {},
   "source": [
    "#### Exercise 2: Use the sorted() function to sort a list of tokens alphabetically with lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "authorized-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'a', 'is', 'string', 'this', 'tokenized']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string = \"this is a tokenized string .\".split()\n",
    "sorted(tokenized_string, key = lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-preview",
   "metadata": {},
   "source": [
    "#### Exercise 3: Use lambda to sort a dictionary of token counts by its value ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "southeast-salmon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dictionary', 1), ('This', 2), ('is', 5), ('a', 10)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"This\": 2, \"is\": 5, \"a\": 10, \"dictionary\": 1}\n",
    "sorted(d.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-relevance",
   "metadata": {},
   "source": [
    "#### Exerise 4: Create a filter that accepts a lambda function and a list of numers as input and that returns all even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aquatic-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = list(range(1,10))\n",
    "f = list(filter(lambda x: x%2==0, nums))\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-berkeley",
   "metadata": {},
   "source": [
    "#### Exercise 5:  Use map() that accepts a lambda function and a list of numbers as input to take the square of all numbers in that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "decent-cause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = list(range(1,10))\n",
    "m = map(lambda x: x*x, nums)\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-locator",
   "metadata": {},
   "source": [
    "## Recursive functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-franchise",
   "metadata": {},
   "source": [
    "#### Write a recursive function counts down from an integer (input) to zero and prints \"Happy new year!\" instead of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "norman-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10!\n",
      "9!\n",
      "8!\n",
      "7!\n",
      "6!\n",
      "5!\n",
      "4!\n",
      "3!\n",
      "2!\n",
      "1!\n",
      "Happy new year!\n"
     ]
    }
   ],
   "source": [
    "def count_down(start):\n",
    "    \n",
    "    print(str(start)+'!')\n",
    "\n",
    "    next = start - 1\n",
    "    if next > 0:\n",
    "        count_down(next)\n",
    "    else:\n",
    "        print(\"Happy new year!\")\n",
    "\n",
    "\n",
    "count_down(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-steps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
